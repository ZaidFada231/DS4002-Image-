{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#MOST OF THIS HAS ALREADY BEEN DEFINED AND EXPLAINED IN THE 1-Preprocessing file, but I am redoing it here for simplicity\n",
    "#define the animal supergroups to use for training and testing\n",
    "animal_supergroups = [\n",
    "    'aquatic_mammals', 'fish', 'insects', 'large_carnivores', \n",
    "    'large_omnivores_and_herbivores', 'medium_mammals', 'non-insect_invertebrates', \n",
    "    'reptiles', 'small_mammals'\n",
    "]\n",
    "\n",
    "#helper function to load CIFAR-100 data\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "#load train and meta data\n",
    "train_data = unpickle('Data/train')\n",
    "meta_data = unpickle('Data/meta')\n",
    "\n",
    "#decode superclass names and find indices of animal supergroups\n",
    "coarse_label_names = [label.decode('utf-8') for label in meta_data[b'coarse_label_names']]\n",
    "animal_indices = [i for i, label in enumerate(coarse_label_names) if label in animal_supergroups]\n",
    "\n",
    "#this is a super function that does a lot of the preprocessing i defined in the last page \n",
    "def preprocess_data(data, animal_indices):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(len(data[b'coarse_labels'])):\n",
    "        if data[b'coarse_labels'][i] in animal_indices:\n",
    "            images.append(data[b'data'][i])\n",
    "            labels.append(data[b'coarse_labels'][i])\n",
    "    #convert to numpy arrays\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    #reshape and normalize image data\n",
    "    images = images.reshape(-1, 3, 32, 32).transpose(0, 2, 3, 1).astype('float32') / 255.0\n",
    "    #map labels to categorical indices\n",
    "    labels = np.array([animal_indices.index(label) for label in labels])\n",
    "    labels = to_categorical(labels, num_classes=len(animal_supergroups))\n",
    "    return images, labels\n",
    "\n",
    "#preprocess train data\n",
    "filtered_images, filtered_labels = preprocess_data(train_data, animal_indices)\n",
    "X_train, X_val, y_train, y_val = train_test_split(filtered_images, filtered_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "#define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu', kernel_initializer='he_uniform'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(animal_supergroups), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#data augmentation, this helps reduce overfitting by \"recreating\" the same image in several distorted ways, such as shifting it up, down, flipping it and so on. This helps fight againt memorization by the model. \n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=15,          \n",
    "    width_shift_range=0.1,      \n",
    "    height_shift_range=0.1,     \n",
    "    horizontal_flip=True        \n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "#fit the model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=64),  \n",
    "    validation_data=(X_val, y_val),               \n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "#plot training history\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#load and preprocess test data\n",
    "test_data = unpickle('Data/test')\n",
    "test_images, test_labels = preprocess_data(test_data, animal_indices)\n",
    "\n",
    "#evaluate the model on test data\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
